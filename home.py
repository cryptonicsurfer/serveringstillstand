from llama_index.llms import OpenAI
from llama_index import VectorStoreIndex, ServiceContext
from llama_index.storage.storage_context import StorageContext
from llama_index.vector_stores.qdrant import QdrantVectorStore
from qdrant_client import QdrantClient
import streamlit as st
# Set up the Qdrant client
url = 'qdrant.utvecklingfalkenberg.se'
api_key = st.secrets["QDRANT_API_KEY"]
client = QdrantClient(url=url, api_key=api_key, port=443)
llm = OpenAI(temperature=0.1, model="gpt-4-1106-preview") #"gpt-4-1106-preview"


# Create QdrantVectorStore
vector_store = QdrantVectorStore(client=client, collection_name="alkohollagen_update2")
# Create the storage and service contexts
storage_context = StorageContext.from_defaults(vector_store=vector_store)
service_context = ServiceContext.from_defaults(llm=llm)
# Build the VectorStoreIndex
index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context, service_context=service_context)
# Create a query engine and query
query_engine = index.as_query_engine(streaming = True, similarity_top_k=3)


# Streamlit UI
st.title("Fr친ga Falkenbergs Serveringstillst친ndsbot :robot_face:")
st.caption('Detta 칛r en bot som hj칛lper till att besvara fr친gor kring serveringstillst친nd. Den baserar sina svar fr친n Folkh칛lsomyndigheten, alkohollagen och Falkenbergskommuns guide kring serveringstillst친nd. 츿ven om den kan en hel del s친 kan det ibland bli fel游뗵')



user_input = st.text_input("**Vad s칬ker svar p친?** *(fr친ga p친 valfritt spr친k :flag-se::flag-eu::flag-gb::earth_africa::es::flag-fi::flag-sy:)")
if user_input:
    # Stream the GPT-4 reply
    response = query_engine.query(f"System instructions: Du hj칛lper anv칛ndare att f칬rst친 information om serveringstillst친nd i Falkenbergs kommun. Du inkluderar URL till de material du anv칛nder som kontext f칬r ditt svar. svara s친 utf칬rligt men enkelt som m칬jligt, g칛rna i punktform om det f칬rb칛ttrar svaret, User query: {user_input}. Du ger alltid k칛llh칛nvisning till URL och du svarar alltid p친 samma spr친k som User query.")
    # with st.sidebar:
    #     for node in response.source_nodes:
    #         url = node.node.metadata['url']
    #         text = node.node.text
    #         score = node.score
    #         st.write(f"URL: {url}, Text: {text}, Score: {score}")
        # st.write(response.source_nodes)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        for chunk in response.response_gen:

            full_response += chunk
            message_placeholder.markdown(full_response)
