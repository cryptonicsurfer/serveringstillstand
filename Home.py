from llama_index.llms import OpenAI
from llama_index import VectorStoreIndex, ServiceContext
from llama_index.storage.storage_context import StorageContext
from llama_index.vector_stores.qdrant import QdrantVectorStore
from qdrant_client import QdrantClient
import streamlit as st

# Replace this URL with the actual URL you want to link to
link = 'https://falkenberg.infocaption.com/players/outlineplayer/print/CreateOutlineHTMLForPrinting.jsp?GuideID=1289&rootNodeIndex=0'
link_text = 'Klicka h칛r f칬r att l칛sa en summerande text om serveringstillst친nd i Falkenbergskommun' #:linked_paperclips: 

# HTML with inline CSS for styling
html_link = f'''
    <a href="{link}" target="_blank">
        <button style="
            background-color: #FFC0CB; /* Pastel Pink */
            border: none;
            color: white;
            padding: 15px 32px;
            text-align: center;
            text-decoration: none;
            display: inline-block;
            font-size: 16px;
            margin: 4px 2px;
            cursor: pointer;
            # border-radius: 15px !important; /* Rounded corners */
            # box-shadow: 3px 3px 5px grey; /* Drop shadow */
        ">{link_text}</button>
    </a>
    '''

st.markdown(html_link, unsafe_allow_html=True)



# Set up the Qdrant client
url = 'qdrant.utvecklingfalkenberg.se'
api_key = st.secrets["QDRANT_API_KEY"]
client = QdrantClient(url=url, api_key=api_key, port=443)
llm = OpenAI(temperature=0.1, model="gpt-4-1106-preview") #"gpt-4-1106-preview"


# Create QdrantVectorStore
vector_store = QdrantVectorStore(client=client, collection_name="alkohollagen_update3")
# Create the storage and service contexts
storage_context = StorageContext.from_defaults(vector_store=vector_store)
service_context = ServiceContext.from_defaults(llm=llm)
# Build the VectorStoreIndex
index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context, service_context=service_context)
# Create a query engine and query
query_engine = index.as_query_engine(streaming = True, similarity_top_k=3)


# Streamlit UI
st.title("Fr친ga Falkenbergs Serveringstillst친ndsbot :robot_face:")
st.caption('Detta 칛r en bot som hj칛lper till att besvara fr친gor kring serveringstillst친nd. Den baserar sina svar fr친n Folkh칛lsomyndigheten, alkohollagen och Falkenbergskommuns guide kring serveringstillst친nd. 츿ven om den kan en hel del, s친 kan det ibland bli fel游뗵 Du kan ocks친 칬va med v친ra Quizar till v칛nster som erbjuds p친 Svenska, Engelska och Arabiska')



user_input = st.text_input("**Vad s칬ker du svar p친?** *(fr친ga p친 valfritt spr친k :flag-se::flag-eu::flag-gb::earth_africa::es::flag-fi::flag-sy:)")
if user_input:
    # Stream the GPT-4 reply
    response = query_engine.query(f"System instructions: Du hj칛lper anv칛ndare att f칬rst친 information om serveringstillst친nd i Falkenbergskommun. Du inkluderar URL till de material du anv칛nder som kontext f칬r ditt svar. svara s친 utf칬rligt men enkelt som m칬jligt, g칛rna i punktform om det f칬rb칛ttrar svaret, User query: {user_input}. Du ger alltid k칛llh칛nvisning till URL och du svarar alltid p친 samma spr친k som User query.")
    # with st.sidebar:
    #     for node in response.source_nodes:
    #         url = node.node.metadata['url']
    #         text = node.node.text
    #         score = node.score
    #         st.write(f"URL: {url}, Text: {text}, Score: {score}")
        # st.write(response.source_nodes)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        for chunk in response.response_gen:

            full_response += chunk
            message_placeholder.markdown(full_response)
